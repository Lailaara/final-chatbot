{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0696e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-10-10 00:21:55.063 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Ishtiyak\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "import streamlit as st\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "# Your OpenAI API Key\n",
    "openai.api_key = \"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\"  # Replace with your OpenAI API key\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_all_pdfs(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:  # Avoid adding None if page has no text\n",
    "                        full_text += page_text\n",
    "                documents[filename] = full_text\n",
    "    return documents\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    embedding_ids = []\n",
    "    chunk_count = 0\n",
    "    for filename, chunks in all_chunks.items():\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "        all_embeddings[filename] = embeddings\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding_ids.append(f\"{filename}-chunk-{i}\")\n",
    "            chunk_count += 1\n",
    "    return all_embeddings, embedding_ids\n",
    "\n",
    "# Function to create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings[next(iter(embeddings))][0].shape[0]  # Embedding size\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    all_embedding_list = []\n",
    "    for embedding_list in embeddings.values():\n",
    "        all_embedding_list.extend(embedding_list)\n",
    "    index.add(np.array(all_embedding_list))\n",
    "    return index\n",
    "\n",
    "# Function to perform FAISS query\n",
    "def query_faiss(query, all_chunks, index, embedding_ids, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[-1])] for i in I[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate response with GPT-3.5\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Function to split text into smaller chunks for translation if necessary\n",
    "def split_text(text, max_tokens=300):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(sentence.split())\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Modified translation function (translates only the response)\n",
    "def translate_text(text, target_language):\n",
    "    text_chunks = split_text(text)\n",
    "    translated_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"Translate this text to {target_language}.\"},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ],\n",
    "            max_tokens=1000  # Adjust based on translation needs\n",
    "        )\n",
    "        translated_chunks.append(response['choices'][0]['message']['content'].strip())\n",
    "    return \" \".join(translated_chunks)\n",
    "\n",
    "# Streamlit interface\n",
    "def main():\n",
    "    st.title(\"RAG-based Chatbot with Document Support\")\n",
    "    user_query = st.text_input(\"Please enter your query:\")\n",
    "    \n",
    "    folder_path = 'C:/Users/Ishtiyak/Desktop/chatbot/documents'  # Folder with your original documents\n",
    "    documents = extract_text_from_all_pdfs(folder_path)\n",
    "    all_chunks = chunk_text_for_all_docs(documents)\n",
    "    all_embeddings, embedding_ids = generate_embeddings_for_all_docs(all_chunks)\n",
    "    index = create_faiss_index(all_embeddings)\n",
    "\n",
    "    if user_query:\n",
    "        retrieved_chunks = query_faiss(user_query, all_chunks, index, embedding_ids)\n",
    "        response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "        st.write(f\"Response in English: {response}\")\n",
    "        \n",
    "        translate_option = st.radio(\"Do you want to translate the response?\", ('No', 'Yes'))\n",
    "        if translate_option == 'Yes':\n",
    "            target_language = st.text_input(\"Enter target language (e.g., 'French', 'Spanish', 'German'):\")\n",
    "            if target_language:\n",
    "                translated_response = translate_text(response, target_language.lower())\n",
    "                st.write(f\"Translated Response in {target_language}: {translated_response}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f897144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
