{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70987244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from openai) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from openai) (3.10.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pinecone-client) (1.26.11)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pdfplumber) (9.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.25.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (2.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.12.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pinecone-client pdfplumber sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a637ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=f\"Summarize this: {text}\",\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "213331e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: AI-Based_Personalized_E-Learning_Systems_Issues_Challenges_and_Solutions.pdf\n",
      "Processing file: coursera report.pdf\n",
      "Processing file: coursera.pdf\n",
      "Processing file: e-learning-in-theory-practice-and-research.pdf\n",
      "Processing file: education-13-01216-v2.pdf\n",
      "Processing file: ELearning-and-a-Case-Study-of-Coursera-and-edX-Online-Platforms.pdf\n",
      "Processing file: E_learning_Concept_Trends.pdf\n",
      "Processing file: HOW E-LEARNING PROGRAMS CAN BE MORE.pdf\n",
      "Processing file: Paper_54-A_Systematic_Literature_Review_on_AI_Algorithms_and_Techniques.pdf\n",
      "Processing file: The Use of AI in ELearning Recommender Systems.pdf\n",
      "Processing file: The_Comparison_of_MOOC_Massive_Open_Online_Course_Platforms_of_edX_and_Coursera_Study_Case_Student_of_Programming_Courses.pdf\n",
      "First 500 characters of AI-Based_Personalized_E-Learning_Systems_Issues_Challenges_and_Solutions.pdf:\n",
      "Received22June2022,accepted12July2022,dateofpublication26July2022,dateofcurrentversion8August2022.\n",
      "DigitalObjectIdentifier10.1109/ACCESS.2022.3193938\n",
      "AI-Based Personalized E-Learning Systems:\n",
      "Issues, Challenges, and Solutions\n",
      "MIRMURTAZA ,YAMNAAHMED,JAWWADAHMEDSHAMSI ,\n",
      "FAHADSHERWANI ,ANDMARIAMUSMAN\n",
      "SystemsResearchLaboratory,NationalUniversityofComputerandEmergingSciences,Karachi75030,Pakistan\n",
      "Correspondingauthor:MirMurtaza(mir.murtaza@nu.edu.pk)\n",
      "ThisworkwassupportedbytheNationalCenterofArtificial\n",
      "First 500 characters of coursera report.pdf:\n",
      "1. Introduction\n",
      "Coursera is a leading online learning platform founded in 2012 by Stanford professors Andrew\n",
      "Ng and Daphne Koller. It collaborates with top universities and organizations to offer courses,\n",
      "certifications, and degree programs across various fields. The platform aims to provide accessible,\n",
      "high-quality education to learners around the globe, enabling them to develop skills for personal\n",
      "and professional growth.\n",
      "2. Key Statistics (2024)\n",
      "• Total Users: 100+ million learners\n",
      "• Course O\n",
      "First 500 characters of coursera.pdf:\n",
      "University of Nebraska - Lincoln\n",
      "DigitalCommons@University of Nebraska - Lincoln\n",
      "Library Philosophy and Practice (e-journal) Libraries at University of Nebraska-Lincoln\n",
      "November 2017\n",
      "Courses beyond borders: A case study of MOOC\n",
      "platform Coursera\n",
      "Huma Shafiq\n",
      "University of Kashmir, huma.msgr14@gmail.com\n",
      "Zahid Ashraf Wani Dr.\n",
      "University of Kashmir, zahidrais@gmail.com\n",
      "Iram Mukhtar Mahajan\n",
      "University of Kashmir, irammahajan6@gmail.com\n",
      "Uzma Qadri\n",
      "University of Kashmir, uzmaqadri10@gmail.com\n",
      "Follow th\n",
      "First 500 characters of e-learning-in-theory-practice-and-research.pdf:\n",
      "E-Learning in Theory, Practice,\n",
      "and Research\n",
      "Maria Janelli\n",
      "Received in Maria Janelli by the American Museum of Natural His-\n",
      "July 2018 Senior Manager of Online Teacher Edu- tory and hosted on the Coursera plat-\n",
      "cation Programs at the American Muse- form. The case study demonstrates both\n",
      "um of Natural History; Ph. D. Fellow at the how learning theory affords a template to\n",
      "City University of New York. Address: 200 guide MOOC creation, and how MOOC\n",
      "Central Park West, New York, NY10024, platforms can\n",
      "First 500 characters of education-13-01216-v2.pdf:\n",
      "education\n",
      "sciences\n",
      "SystematicReview\n",
      "Adaptive Learning Using Artificial Intelligence in e-Learning:\n",
      "A Literature Review\n",
      "IlieGligorea1,2,* ,MariusCioca3 ,RomanaOancea1,Andra-TeodoraGorski4 ,HortensiaGorski1\n",
      "andPaulTudorache5\n",
      "1 DepartmentofTechnicalSciences,FacultyofMilitaryManagement,“NicolaeBălcescu”LandForces\n",
      "Academy,550170Sibiu,Romania;oancea.romana@gmail.com(R.O.);hortensia.gorski@gmail.com(H.G.)\n",
      "2 DoctoralSchool,UniversityofPetrosani,332006Petrosani,Romania\n",
      ", ,\n",
      "3 DepartmentofIndustrialEnginee\n",
      "First 500 characters of ELearning-and-a-Case-Study-of-Coursera-and-edX-Online-Platforms.pdf:\n",
      "This article is part of the thematic issue in the framework of the Erasmus + Jean Monnet Centre of\n",
      "Excellence: Strategic Observatory for Europe 2030 project, edited by prof. Borut Rončević.\n",
      "The project was Co-funded by the Erasmus+ programme of the European Union, Project number:\n",
      "611564-EPP-1-2019-1-SI-EPPJMO-CoE.\n",
      "Open Access. © 2022 Anja Likovič. This is an open access article distributed under the Creative Commons Attribution-\n",
      "NonCommercial-NoDerivs license (https://creativecommons.org/license\n",
      "First 500 characters of E_learning_Concept_Trends.pdf:\n",
      "E-learning Concept Trends\n",
      "Manuela Aparicio Fernando Bacao\n",
      "Instituto Universitario de Lisboa\n",
      "ISCTE-IUL, Adetti-IUL ISEGI, Universidade Nova de\n",
      "ISEGI, Universidade Nova de Lisboa\n",
      "Lisboa Lisboa, Portugal\n",
      "IADE/ UNIDCOM\n",
      "Lisboa, Portugal bacao@isegi.unl.pt\n",
      "manuela.aparicio@acm.org\n",
      "ABSTRACT E-learning industry increased significantly in terms of usage, in\n",
      "participation gathering new markets and designing new business\n",
      "E-learning systems are widely used from academia to industry.\n",
      "models for universities \n",
      "First 500 characters of HOW E-LEARNING PROGRAMS CAN BE MORE.pdf:\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/372252223\n",
      "HOW E-LEARNING PROGRAMS CAN BE MORE INDIVIDUALIZED WITH\n",
      "ARTIFICIAL INTELLIGENCE -A THEORETICAL APPROACH FROM A\n",
      "PEDAGOGICAL POINT OF VIEW\n",
      "Article in Muallim Journal of Social Science and Humanities · July 2023\n",
      "DOI: 10.33306/mjssh/240\n",
      "CITATIONS READS\n",
      "10 676\n",
      "4 authors:\n",
      "Nele Rohde Nicole Flindt\n",
      "Pädagogische Hochschule Heidelberg Pädagogische Hochschule Heidelberg\n",
      "3 PUBLICATIONS 10 \n",
      "First 500 characters of Paper_54-A_Systematic_Literature_Review_on_AI_Algorithms_and_Techniques.pdf:\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/369050789\n",
      "A Systematic Literature Review on AI Algorithms and Techniques Adopted by e-\n",
      "Learning Platforms for Psychological and Emotional States\n",
      "Article in International Journal of Advanced Computer Science and Applications · January 2023\n",
      "DOI: 10.14569/IJACSA.2023.0140254\n",
      "CITATIONS READS\n",
      "3 393\n",
      "1 author:\n",
      "Lubna A. Alharbi\n",
      "University of Tabuk\n",
      "23 PUBLICATIONS 108 CITATIONS\n",
      "SEE PROFILE\n",
      "All co\n",
      "First 500 characters of The Use of AI in ELearning Recommender Systems.pdf:\n",
      "Available online at www.sciencedirect.com\n",
      "Available online at www.sciencedirect.com\n",
      "ScienceDirect\n",
      "Available onSlincei eatn wcwew.Dsciiernececdtirect.com\n",
      "Procedia Computer Science 00 (2023) 000–000\n",
      "ScienceDirect\n",
      "Procedia Computer Science 00 (2023) 000–000\n",
      "www.elsevier.com/locate/procedia\n",
      "www.elsevier.com/locate/procedia\n",
      "Procedia Computer Science 224 (2023) 437–442\n",
      "The 10th International Symposium on Emerging Inter-networks, Communication and Mobility\n",
      "The 10th InternationaAl Sugyumspt o1s4i-u1m6, \n",
      "First 500 characters of The_Comparison_of_MOOC_Massive_Open_Online_Course_Platforms_of_edX_and_Coursera_Study_Case_Student_of_Programming_Courses.pdf:\n",
      "The Comparison of MOOC (Massive Open Online\n",
      "Course) Platforms of edX and Coursera\n",
      "(study case: student of programming courses)\n",
      "Tanty Oktavia Harjanto Prabowo Meyliana\n",
      "Information Systems Department, Computer Science Department, Information Systems Department,\n",
      "School of Information Systems, BINUS Graduate Program - Doctor of School of Information Systems\n",
      "Computer Science Department, Computer Science Bina Nusantara University\n",
      "BINUS Graduate Program - Doctor of Bina Nusantara University Jakarta, In\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# Define the folder where your PDFs are located\n",
    "folder_path = 'C:/Users/Ishtiyak/Desktop/chatbot/documents'\n",
    "\n",
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_all_pdfs(folder_path):\n",
    "    documents = {}\n",
    "    \n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    full_text += page.extract_text()\n",
    "            documents[filename] = full_text\n",
    "    return documents\n",
    "\n",
    "# Extract text from all PDFs\n",
    "documents = extract_text_from_all_pdfs(folder_path)\n",
    "\n",
    "# Check the extracted text from one of the documents\n",
    "for filename, text in documents.items():\n",
    "    print(f\"First 500 characters of {filename}:\")\n",
    "    print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b906a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ishtiyak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-Based_Personalized_E-Learning_Systems_Issues_Challenges_and_Solutions.pdf has 16 chunks.\n",
      "coursera report.pdf has 3 chunks.\n",
      "coursera.pdf has 11 chunks.\n",
      "e-learning-in-theory-practice-and-research.pdf has 12 chunks.\n",
      "education-13-01216-v2.pdf has 8 chunks.\n",
      "ELearning-and-a-Case-Study-of-Coursera-and-edX-Online-Platforms.pdf has 21 chunks.\n",
      "E_learning_Concept_Trends.pdf has 9 chunks.\n",
      "HOW E-LEARNING PROGRAMS CAN BE MORE.pdf has 16 chunks.\n",
      "Paper_54-A_Systematic_Literature_Review_on_AI_Algorithms_and_Techniques.pdf has 20 chunks.\n",
      "The Use of AI in ELearning Recommender Systems.pdf has 8 chunks.\n",
      "The_Comparison_of_MOOC_Massive_Open_Online_Course_Platforms_of_edX_and_Coursera_Study_Case_Student_of_Programming_Courses.pdf has 8 chunks.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to chunk text for all documents\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    \n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        \n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        \n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Chunk the text for all documents\n",
    "all_chunks = chunk_text_for_all_docs(documents)\n",
    "\n",
    "# Example: Print the number of chunks for one document\n",
    "for filename, chunks in all_chunks.items():\n",
    "    print(f\"{filename} has {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551493ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for AI-Based_Personalized_E-Learning_Systems_Issues_Challenges_and_Solutions.pdf\n",
      "Generating embeddings for coursera report.pdf\n",
      "Generating embeddings for coursera.pdf\n",
      "Generating embeddings for e-learning-in-theory-practice-and-research.pdf\n",
      "Generating embeddings for education-13-01216-v2.pdf\n",
      "Generating embeddings for ELearning-and-a-Case-Study-of-Coursera-and-edX-Online-Platforms.pdf\n",
      "Generating embeddings for E_learning_Concept_Trends.pdf\n",
      "Generating embeddings for HOW E-LEARNING PROGRAMS CAN BE MORE.pdf\n",
      "Generating embeddings for Paper_54-A_Systematic_Literature_Review_on_AI_Algorithms_and_Techniques.pdf\n",
      "Generating embeddings for The Use of AI in ELearning Recommender Systems.pdf\n",
      "Generating embeddings for The_Comparison_of_MOOC_Massive_Open_Online_Course_Platforms_of_edX_and_Coursera_Study_Case_Student_of_Programming_Courses.pdf\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    for filename, chunks in all_chunks.items():\n",
    "        print(f\"Generating embeddings for {filename}\")\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "        all_embeddings[filename] = embeddings\n",
    "    return all_embeddings\n",
    "\n",
    "# Generate embeddings\n",
    "all_embeddings = generate_embeddings_for_all_docs(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ed7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.24.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from faiss-cpu) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from packaging->faiss-cpu) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b51c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query:what is Ai\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3875408431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrag_chatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3875408431.py\u001b[0m in \u001b[0;36mrag_chatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Retrieve the most relevant chunks using FAISS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mretrieved_chunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_faiss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Generate the response using GPT-3.5 Turbo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3875408431.py\u001b[0m in \u001b[0;36mquery_faiss\u001b[1;34m(query, top_k)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Perform the FAISS search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure the embedding is passed as a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Retrieve the matching text chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to retrieve the most relevant chunks using FAISS\n",
    "def query_faiss(query, top_k=3):\n",
    "    # Generate embedding for the query using SentenceTransformer\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Perform the FAISS search\n",
    "    D, I = index.search(np.array(query_embedding), top_k)  # Ensure the embedding is passed as a numpy array\n",
    "    \n",
    "    # Retrieve the matching text chunks\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[1])] for i in I[0]]\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate response with context\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\\n\\nAnswer:\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers based on the provided information.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200  # Adjust the max_tokens if needed\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Main function to interact with the chatbot\n",
    "def rag_chatbot():\n",
    "    # Take query as input from the user\n",
    "    user_query = input(\"Please enter your query: \")\n",
    "    \n",
    "    # Retrieve the most relevant chunks using FAISS\n",
    "    retrieved_chunks = query_faiss(user_query)\n",
    "    \n",
    "    # Generate the response using GPT-3.5 Turbo\n",
    "    response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "response = rag_chatbot()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bec9abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     -------------------------------------- 981.5/981.5 kB 2.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=d8c13c1406697fab38594f8d3d628cc47baf998188f42c2ad51b724c7a8a3238\n",
      "  Stored in directory: c:\\users\\ishtiyak\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2decf73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query:what is AI\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\2906714906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0muser_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please enter your query: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtranslated_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate_text_to_italian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslated_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "def translate_text_to_italian(text):\n",
    "    # Detect the language of the input text\n",
    "    detected_language = detect(text)\n",
    "    \n",
    "    # If the text is already in Italian, return it\n",
    "    if detected_language == \"it\":\n",
    "        return text\n",
    "    \n",
    "    # Otherwise, translate to Italian\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text into Italian.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Translate this text to Italian: {text}\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Example usage\n",
    "user_query = input(\"Please enter your query: \")\n",
    "translated_query = translate_text_to_italian(response)\n",
    "print(translated_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4fdc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query:what is Ai\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-proj-********************************************************************************************************************************************************d1sA. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3309940813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;31m# Run the chatbot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m \u001b[0mrag_chatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3309940813.py\u001b[0m in \u001b[0;36mrag_chatbot\u001b[1;34m(all_chunks, index, embedding_ids)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;31m# Generate response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrieved_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Response in English: {response}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11228\\3309940813.py\u001b[0m in \u001b[0;36mgenerate_response_with_context\u001b[1;34m(query, retrieved_chunks)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_response_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrieved_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         messages=[\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         )\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             return (\n\u001b[1;32m--> 700\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-proj-********************************************************************************************************************************************************d1sA. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "\n",
    "# Your OpenAI API Key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_all_pdfs(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    full_text += page.extract_text()\n",
    "                documents[filename] = full_text\n",
    "    return documents\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    embedding_ids = []\n",
    "    chunk_count = 0\n",
    "    for filename, chunks in all_chunks.items():\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "        all_embeddings[filename] = embeddings\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding_ids.append(f\"{filename}-chunk-{i}\")\n",
    "            chunk_count += 1\n",
    "    return all_embeddings, embedding_ids\n",
    "\n",
    "# Function to create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings[next(iter(embeddings))][0].shape[0]  # Embedding size\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    all_embedding_list = []\n",
    "    for embedding_list in embeddings.values():\n",
    "        all_embedding_list.extend(embedding_list)\n",
    "    index.add(np.array(all_embedding_list))\n",
    "    return index\n",
    "\n",
    "# Function to perform FAISS query\n",
    "def query_faiss(query, all_chunks, index, embedding_ids, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[-1])] for i in I[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate response with GPT-3.5\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Function to translate the text\n",
    "def translate_text(text, target_language):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Translate this text to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Main chatbot function\n",
    "def rag_chatbot(all_chunks, index, embedding_ids):\n",
    "    user_query = input(\"Please enter your query: \")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = query_faiss(user_query, all_chunks, index, embedding_ids)\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "    print(f\"Response in English: {response}\")\n",
    "    \n",
    "    # Ask user for language preference\n",
    "    translate_option = input(\"Do you want to translate the response? (yes/no): \")\n",
    "    if translate_option.lower() == 'yes':\n",
    "        target_language = input(\"Enter target language (e.g., 'French', 'Spanish', 'German'): \").lower()\n",
    "        translated_response = translate_text(response, target_language)\n",
    "        print(f\"Translated Response in {target_language}: {translated_response}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'C:/Users/Ishtiyak/Desktop/chatbot/documents'  # Update this with your actual folder path\n",
    "documents = extract_text_from_all_pdfs(folder_path)\n",
    "all_chunks = chunk_text_for_all_docs(documents)\n",
    "all_embeddings, embedding_ids = generate_embeddings_for_all_docs(all_chunks)\n",
    "index = create_faiss_index(all_embeddings)\n",
    "\n",
    "# Run the chatbot\n",
    "rag_chatbot(all_chunks, index, embedding_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f8a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
