{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d69a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your query: what is e learning\n",
      "Response in English: E-learning, short for electronic learning, refers to the use of technology to deliver education and training programs. It allows learners to access educational content and materials anytime and anywhere through the internet. E-learning systems provide a platform for interactive learning experiences, communication between students and teachers, and collaboration among peers. The concept of e-learning has evolved over time, incorporating different perspectives such as pedagogical models, instructional strategies, and learning technologies. E-learning is commonly used in various settings, including academia and industry, to facilitate continuous learning and skill development.\n",
      "Do you want to translate the response? (yes/no): yes\n",
      "Enter target language (e.g., 'French', 'Spanish', 'German'): bengali\n",
      "Translated Response in bengali: ই-লার্নিং, ইলেক্ট্রনিক শেখানোর সংক্ষিপ্ত রূপ, শিক্ষা এবং প্রশিক্ষণ প্রোগ্রাম প্রদানে প্রযুক্তি ব্যবহার করা বোঝায়। এটা শিক্ষার্থীদেরকে ইন্টারনেটের মাধ্যমে শিক্ষামূলক সামগ্রী ও উপাদান অ্যাক্সেস করার সুযোগ দেয়। ই-লার্নিং সিস্টেম ইন্টারেক্টিভ লার্নিং অভিজ্ঞতা, শিক্ষার্থীদের ও শিক্ষকদের মধ্যে যোগাযোগ, এবং সহখ্য মধ্যমে সহযোগিতা প্রদান করার জন্য একটি ভূমিকা প্রদান করে। ই-লার্নিং এর ধারণা সময়ের সাথে বিকাশ করে এসেছে, যেমন- শিক্ষা মডেল, শিক্ষাবিধি, এবং শেখার প্রযুক্তি প্রক্রিয়া। ই-লার্নিংটি আবাসিক শিক্ষাবিদ্যালয় এবং শিল্প সংস্থান সহ বিভিন্ন সেটিংসে প্রয়োজনীয় লার্নিং এবং দক্ষতা উন্নতি সুবিধা দেওয়ার জন্য সাধারণভাবে ব্যবহৃত হয়।\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "\n",
    "# Your OpenAI API Key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_all_pdfs(folder_path):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    full_text += page.extract_text()\n",
    "                documents[filename] = full_text\n",
    "    return documents\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    embedding_ids = []\n",
    "    chunk_count = 0\n",
    "    for filename, chunks in all_chunks.items():\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "        all_embeddings[filename] = embeddings\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding_ids.append(f\"{filename}-chunk-{i}\")\n",
    "            chunk_count += 1\n",
    "    return all_embeddings, embedding_ids\n",
    "\n",
    "# Function to create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings[next(iter(embeddings))][0].shape[0]  # Embedding size\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    all_embedding_list = []\n",
    "    for embedding_list in embeddings.values():\n",
    "        all_embedding_list.extend(embedding_list)\n",
    "    index.add(np.array(all_embedding_list))\n",
    "    return index\n",
    "\n",
    "# Function to perform FAISS query\n",
    "def query_faiss(query, all_chunks, index, embedding_ids, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[-1])] for i in I[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate response with GPT-3.5\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\n\\nRelevant information from documents:\\n{retrieved_chunks}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Function to split text into smaller chunks for translation if necessary\n",
    "def split_text(text, max_tokens=300):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(sentence.split())\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Modified translation function (translates only the response)\n",
    "def translate_text(text, target_language):\n",
    "    text_chunks = split_text(text)\n",
    "    translated_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"Translate this text to {target_language}.\"},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ],\n",
    "            max_tokens=1000  # Adjust based on translation needs\n",
    "        )\n",
    "        translated_chunks.append(response['choices'][0]['message']['content'].strip())\n",
    "    return \" \".join(translated_chunks)\n",
    "\n",
    "# Main chatbot function\n",
    "def rag_chatbot(all_chunks, index, embedding_ids):\n",
    "    user_query = input(\"Please enter your query: \")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = query_faiss(user_query, all_chunks, index, embedding_ids)\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "    print(f\"Response in English: {response}\")\n",
    "    \n",
    "    # Ask user for language preference\n",
    "    translate_option = input(\"Do you want to translate the response? (yes/no): \")\n",
    "    if translate_option.lower() == 'yes':\n",
    "        target_language = input(\"Enter target language (e.g., 'French', 'Spanish', 'German'): \").lower()\n",
    "        translated_response = translate_text(response, target_language)\n",
    "        print(f\"Translated Response in {target_language}: {translated_response}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'C:/Users/Ishtiyak/Desktop/chatbot/documents'  # Folder with your original documents\n",
    "documents = extract_text_from_all_pdfs(folder_path)\n",
    "all_chunks = chunk_text_for_all_docs(documents)\n",
    "all_embeddings, embedding_ids = generate_embeddings_for_all_docs(all_chunks)\n",
    "index = create_faiss_index(all_embeddings)\n",
    "\n",
    "# Run the chatbot\n",
    "rag_chatbot(all_chunks, index, embedding_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f281ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (1.30.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (5.3.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (0.8.1b0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (4.23.4)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (5.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (13.7.0)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (4.11.3)\n",
      "Requirement already satisfied: validators<1,>=0.2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (0.22.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (3.1.41)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: pyarrow>=6.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (17.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from streamlit) (1.4.4)\n",
      "Requirement already satisfied: toolz in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.11.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (2.11.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from packaging<24,>=16.8->streamlit) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from tzlocal<6,>=1.1->streamlit) (2023.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (21.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ishtiyak\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a86f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
