{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d05381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Streamlit code into a file\n",
    "code = \"\"\"\n",
    "import os\n",
    "import streamlit as st\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Your OpenAI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Streamlit UI for the chatbot\n",
    "st.set_page_config(page_title=\"Document-Based Chatbot\", layout=\"wide\")\n",
    "\n",
    "st.title(\"Document-Based Question Answering Chatbot\")\n",
    "st.markdown(\"Upload your documents and ask questions!\")\n",
    "\n",
    "# Sidebar for translation option\n",
    "with st.sidebar:\n",
    "    st.header(\"Translation Options\")\n",
    "    translate_option = st.radio(\"Would you like to translate responses?\", ('No', 'Yes'))\n",
    "    target_language = st.selectbox(\"Select target language\", ['French', 'Spanish', 'German']) if translate_option == 'Yes' else None\n",
    "\n",
    "# Initialize session state for the chat history\n",
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state['chat_history'] = []\n",
    "\n",
    "# File Uploader\n",
    "uploaded_files = st.file_uploader(\"Upload PDF documents\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "@st.cache\n",
    "def extract_text_from_pdfs(files):\n",
    "    documents = {}\n",
    "    for file in files:\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                full_text += page.extract_text()\n",
    "            documents[file.name] = full_text\n",
    "    return documents\n",
    "\n",
    "# Function to chunk the text into smaller segments\n",
    "def chunk_text_for_all_docs(documents, max_tokens=500):\n",
    "    all_chunks = {}\n",
    "    for filename, text in documents.items():\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        tokens_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(sentence.split())\n",
    "            if tokens_count + tokens > max_tokens:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []\n",
    "                tokens_count = 0\n",
    "            chunk.append(sentence)\n",
    "            tokens_count += tokens\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        all_chunks[filename] = chunks\n",
    "    return all_chunks\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings_for_all_docs(all_chunks):\n",
    "    all_embeddings = {}\n",
    "    embedding_ids = []\n",
    "    chunk_count = 0\n",
    "    for filename, chunks in all_chunks.items():\n",
    "        embeddings = model.encode(chunks, convert_to_tensor=False)\n",
    "        all_embeddings[filename] = embeddings\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding_ids.append(f\"{filename}-chunk-{i}\")\n",
    "            chunk_count += 1\n",
    "    return all_embeddings, embedding_ids\n",
    "\n",
    "# Function to create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings[next(iter(embeddings))][0].shape[0]  # Embedding size\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    all_embedding_list = []\n",
    "    for embedding_list in embeddings.values():\n",
    "        all_embedding_list.extend(embedding_list)\n",
    "    index.add(np.array(all_embedding_list))\n",
    "    return index\n",
    "\n",
    "# Function to query FAISS\n",
    "def query_faiss(query, all_chunks, index, embedding_ids, top_k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_chunks = [all_chunks[embedding_ids[i].split('-chunk-')[0]][int(embedding_ids[i].split('-chunk-')[-1])] for i in I[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Function to generate a response from GPT-3.5\n",
    "def generate_response_with_context(query, retrieved_chunks):\n",
    "    prompt = f\"User query: {query}\\\\n\\\\nRelevant information from documents:\\\\n{retrieved_chunks}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Function to translate the response\n",
    "def translate_text(text, target_language):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Translate this text to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Main section for query and responses\n",
    "if uploaded_files:\n",
    "    with st.spinner('Processing documents...'):\n",
    "        documents = extract_text_from_pdfs(uploaded_files)\n",
    "        all_chunks = chunk_text_for_all_docs(documents)\n",
    "        all_embeddings, embedding_ids = generate_embeddings_for_all_docs(all_chunks)\n",
    "        index = create_faiss_index(all_embeddings)\n",
    "        st.success('Documents processed successfully!')\n",
    "\n",
    "    # Continuous Chat Interface\n",
    "    user_query = st.text_input(\"Enter your query:\")\n",
    "\n",
    "    if st.button(\"Submit Query\") and user_query:\n",
    "        retrieved_chunks = query_faiss(user_query, all_chunks, index, embedding_ids)\n",
    "        response = generate_response_with_context(user_query, retrieved_chunks)\n",
    "        \n",
    "        # Store the chat history\n",
    "        st.session_state.chat_history.append(f\"You: {user_query}\")\n",
    "        st.session_state.chat_history.append(f\"Chatbot: {response}\")\n",
    "        \n",
    "        # Display chat history\n",
    "        for msg in st.session_state.chat_history:\n",
    "            st.write(msg)\n",
    "\n",
    "        # Handle translation if enabled\n",
    "        if translate_option == 'Yes' and target_language:\n",
    "            translated_response = translate_text(response, target_language)\n",
    "            st.write(f\"Translated Response ({target_language}): {translated_response}\")\n",
    "\n",
    "\"\"\"\n",
    "with open(\"app.py\", \"w\") as file:\n",
    "    file.write(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5db8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run apppp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906a7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
